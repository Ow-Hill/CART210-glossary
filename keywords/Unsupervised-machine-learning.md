## Unsupervised machine learning 

Unsupervised machine learning is when algorithms analyze data without human or supervisory intervention [^OwenHUnsupervisedMachineLearning1].  What is significant is that there is not a pre-defined outcome, or a ‘correct’ answer, rather the algorithms instead look for relationships or ‘clusters’ within the data sets. With the use cases of unsupervised learning being object segmentation, similarity detection, and automatic labeling to name a few. [^OwenHUnsupervisedMachineLearning1] 

There are four main ways to cluster data [^OwenHUnsupervisedMachineLearning1] 

K-means [^OwenHUnsupervisedMachineLearning1], this is the classical approach to clustering data where k clusters are optimized to minimize the Euclidean distance from each point to the center point of the cluster. One major drawback is that the distance is a radial function, meaning that k-means assumes all clusters are convex and cannot solve problems where the shapes have deep concavities (e.g. The horseshoe problem). K-means use cases are broad and are applicable to convex datasets. 

DBSCAN [^OwenHUnsupervisedMachineLearning1], this approach is one of the methods used to solve sets with concavities. DBSCAN works by analyzing the difference between points surrounded by other samples and boundary samples (samples where the valences of two or more clusters overlap). It uses this to determine high density areas (which become clusters) and low-density areas. This method makes no assumptions about the shape or number of clusters and tuning the parameters for this method is often required. 

Spectral Clustering [^OwenHUnsupervisedMachineLearning1], this is the other method for dealing with concavities and is based on measuring the affinity among samples. It uses k-means on subspaces generated by the Laplacian of the affinity matrix (a relation on the given set represented as a matrix to see where and how gradients diverge as well as dealing with boundary points). It exploits the power of kernel functions to find the affinity between points; by doing this it can find clusters where simple distance cannot be classified correctly. The use case for spectral clustering is when other methods fail to separate the dataset correctly as well as it being very efficient for image segmentation. 

Hierarchical clustering [^OwenHUnsupervisedMachineLearning1] is the fourth and final type of clustering. It works by splitting and merging clusters until a final configuration is reached, it also shares high level conceptual similarities with merge sort in how they merge elements. What was mainly covered is Agglomerative Hierarchical clustering. The method begins by treating each sample as its own cluster, it then merges clusters until the desired amount of them is reached. It merges clusters based on two functions: Metric function (Affinity), being the distance between elements (element of k-means), and the linkage criterion, the target function to determine what clusters must be merged next. 

 

There is a long history in the development of this field, which today, is critical as a tool in the execution of artificial intelligence.  In the late 1930’s Robert Tyson [^OwenHUnsupervisedMachineLearning2], a behavioral psychologist, experimented with statistical techniques to find patterns in multivariate data without predefined categories using techniques such as factor analysis [^OwenHUnsupervisedMachineLearning2]. This led to the development of cluster analysis and foundational work in data science that helped scaffold for the development of more complex systems 

Clustering analysis was also very important in the development of this field and was pioneered by Stuart Lloyd at Bell Labs in 1957 using k-means clustering algorithm (interesting to note that this was not published until 1982) [^OwenHUnsupervisedMachineLearning5].  In the 1970’s and 1980’s hierarchical clustering methods were pursued; Teuvo Kohonen was prominent in this field and developed self-organizing maps [^OwenHUnsupervisedMachineLearning6].  Development continued into the 2000’s with pursuit of neural network and deep learning approaches to unsupervised learning and more sophisticated algorithms.  In the last decade there has been explosive growth in this field with innovations such as variational autoencoders (VAE’s), generative adversarial networks (GAN’s) and other self-supervised learning approaches [^OwenHUnsupervisedMachineLearning7].  

The ability to evaluate very large pools of data without a pre-set answer has been very useful in current times [^OwenHUnsupervisedMachineLearning2]   [^OwenHUnsupervisedMachineLearning3]  [^OwenHUnsupervisedMachineLearning4].  This has allowed marketing analysis to be tailored for individual users rather than a pre-determined outcome [^OwenHUnsupervisedMachineLearning3] [^OwenHUnsupervisedMachineLearning4].  For example, these types of analysis can be used by e-commerce platforms to anticipate what other products an individual may be interested in purchasing.  We are all familiar with ‘personalized’ ads which, based on unsupervised machine learning, the system attempts to anticipate the next product we want to purchase by clustering users and through that creating a personalized recommender for each user  [^OwenHUnsupervisedMachineLearning3]. Through these recommendation systems we also see a shift from demographic based advertising to behavioral targeting with advertising  [^OwenHUnsupervisedMachineLearning3]  [^OwenHUnsupervisedMachineLearning4], allowing unsupervised machine learning to filter and curate what we consume to maximize retention rates and customer satisfaction. These techniques can identify patterns in media like music and art, enabling them to create similar content.  They can also look at various other contexts and look for ‘patterns’ or ‘signals’ in the data and flag that for evaluation; examples of this is financial market analysis and content moderation on social media forums [^OwenHUnsupervisedMachineLearning3]. With so much of our perception based on what we know and are exposed to, these recommendation systems, intentionally or unintentionally, are able directly to shape our narrative of which we see the world through. Unsupervised machine learning will also help change the way we interact online, with one such example being how it can be employed to add a better image-based search function for e-commerce [^OwenHUnsupervisedMachineLearning2]. It can also inform decision-making on how to best create in person interactive experiences [^OwenHUnsupervisedMachineLearning4]. It is evident how unsupervised machine learning shapes our world and society in many ways, from content moderation and recommendation, to shaping how we interact with experiences both physical and digital. 

 

 

[^OwenHUnsupervisedMachineLearning1]: Bonaccorso, Giuseppe, O’Reilly for Higher Education (Firm), and Safari, an O’Reilly Media Company. 2020. Mastering Machine Learning Algorithms - Second Edition. 2nd edition. Packt Publishing. https://balasahebtarle.wordpress.com/wp-content/uploads/2020/01/machine-learning-algorithms_text-book.pdf  

[^OwenHUnsupervisedMachineLearning2]: Addagarla, Ssvr Kumar, and Anthoniraj Amalanathan. 2020. "Probabilistic Unsupervised Machine Learning Approach for a Similar Image Recommender System for E-Commerce" Symmetry 12, no. 11: 1783. https://doi.org/10.3390/sym12111783.  

[^OwenHUnsupervisedMachineLearning3]: Cintia Ganesha Putri, Debby, Jenq-Shiou Leu, and Pavel Seda. "Design of an unsupervised machine learning-based movie recommender system." Symmetry 12, no. 2 (2020): 185. https://www.mdpi.com/2073-8994/12/2/185  

[^OwenHUnsupervisedMachineLearning4]: Piccialli, Francesco, Giampaolo Casolla, Salvatore Cuomo, Fabio Giampaolo, Edoardo Prezioso, and Vincenzo Schiano Di Cola. "Unsupervised learning on multimedia data: a Cultural Heritage case study." Multimedia tools and applications 79 (2020): 34429-34442. https://link.springer.com/article/10.1007/s11042-020-08781-1 

[^OwenHUnsupervisedMachineLearning5]: Lloyd, Stuart. 1982. “Least Squares Quantization in PCM.” IEEE Transactions on Information Theory. https://hal.science/hal-04614938/document. 

[^OwenHUnsupervisedMachineLearning6]: Kohonen, Teuvo and Department of Technical Physics, Helsinki University of Technology, Espoo, Finland. 1982. “Self-Organized Formation of Topologically Correct Feature Maps.” Biological Cybernetics 43: 59–69. https://www.cnbc.cmu.edu/~tai/nc19journalclubs/Kohonen1982_Article_Self-organizedFormationOfTopol.pdf. 

[^OwenHUnsupervisedMachineLearning7]: Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. “Generative Adversarial Networks.” Communications of the ACM 63 (11): 139–44. https://doi.org/10.1145/3422622. 
